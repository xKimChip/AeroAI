{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DH5gs2LTOfaG",
        "outputId": "c9404507-2ebf-4998-e25b-92a452defdee"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7fR4tX30Obbk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eaaed77-8ffa-47e4-f0d0-0ca99b871281"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preprocessing data...\n",
            "\n",
            "Initial entries: 39092\n",
            "Found 5507 missing/invalid values in alt\n",
            "Filled missing values with median: 32275.0\n",
            "Found 5491 missing/invalid values in gs\n",
            "Filled missing values with median: 416.0\n",
            "Found 3 missing/invalid values in heading\n",
            "Filled missing values with median: 160.0\n",
            "Found 1 missing/invalid values in lat\n",
            "Filled missing values with median: 25.54697\n",
            "Found 1 missing/invalid values in lon\n",
            "Filled missing values with median: 50.64428\n",
            "Found 5850 missing/invalid values in vertRate\n",
            "Filled missing values with median: 0.0\n",
            "\n",
            "Final entries: 39092\n",
            "\n",
            "Training model...\n",
            "Epoch [10/100], Loss: 0.045026, Val Loss: 0.010651\n",
            "Epoch [20/100], Loss: 0.035528, Val Loss: 0.009434\n",
            "Epoch [30/100], Loss: 0.030572, Val Loss: 0.007979\n",
            "Epoch [40/100], Loss: 0.026876, Val Loss: 0.007803\n",
            "Epoch [50/100], Loss: 0.024194, Val Loss: 0.007689\n",
            "Epoch [60/100], Loss: 0.023131, Val Loss: 0.006789\n",
            "Epoch [70/100], Loss: 0.021445, Val Loss: 0.006821\n",
            "Epoch [80/100], Loss: 0.019502, Val Loss: 0.005889\n",
            "Epoch [90/100], Loss: 0.019075, Val Loss: 0.005586\n",
            "Epoch [100/100], Loss: 0.018459, Val Loss: 0.005366\n",
            "\n",
            "Generating synthetic anomalies...\n",
            "\n",
            "Created 100 synthetic anomalies\n",
            "\n",
            "Evaluating anomaly detection...\n",
            "\n",
            "Set anomaly threshold at: 0.018506\n",
            "\n",
            "Anomaly Detection Performance:\n",
            "False Positives: 281 out of 7819 normal points (3.59%)\n",
            "True Positives: 93 out of 100 anomalies (93.00%)\n",
            "\n",
            "Reconstruction Error Statistics:\n",
            "Normal data - Mean: 0.005366, Std: 0.006776\n",
            "Anomaly data - Mean: 0.841476, Std: 0.688165\n",
            "\n",
            "Example: Detecting anomalies in sample data...\n",
            "Sample 1: Normal\n",
            "Sample 2: Normal\n",
            "Sample 3: Normal\n",
            "Sample 4: Normal\n",
            "Sample 5: Anomaly\n",
            "\n",
            "Saving model...\n",
            "\n",
            "Model saved successfully\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import json\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "class FlightPredictionMLP(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(FlightPredictionMLP, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_size, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(64, 32),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(32, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(64, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(128, input_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "def minimal_preprocess(filepath: str):\n",
        "    \"\"\"Minimal preprocessing - only handle format issues\"\"\"\n",
        "    with open(filepath, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    initial_count = len(df)\n",
        "    print(f\"\\nInitial entries: {initial_count}\")\n",
        "\n",
        "    features = ['alt', 'gs', 'heading', 'lat', 'lon', 'vertRate']\n",
        "\n",
        "    # Convert to numeric, keeping all values\n",
        "    for col in features:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "            nan_count = df[col].isna().sum()\n",
        "            if nan_count > 0:\n",
        "                print(f\"Found {nan_count} missing/invalid values in {col}\")\n",
        "                median_val = df[col].median()\n",
        "                df[col] = df[col].fillna(median_val)\n",
        "                print(f\"Filled missing values with median: {median_val}\")\n",
        "\n",
        "    # Handle altChange encoding\n",
        "    df['altChange'] = df['altChange'].fillna(' ')\n",
        "    df['altChange_encoded'] = df['altChange'].map({' ': 0, 'C': 1, 'D': -1}).fillna(0)\n",
        "\n",
        "    print(f\"\\nFinal entries: {len(df)}\")\n",
        "    return df\n",
        "\n",
        "def create_synthetic_anomalies(df, num_anomalies=100):\n",
        "    \"\"\"Generate synthetic anomalies with multiple modifications per anomaly\"\"\"\n",
        "    anomalies = []\n",
        "    for _ in range(num_anomalies):\n",
        "        base_flight = df.sample(n=1).iloc[0].to_dict()\n",
        "        anomaly = base_flight.copy()\n",
        "\n",
        "        # Apply multiple modifications to each anomaly\n",
        "        num_modifications = np.random.randint(1, 4)  # 1-3 modifications\n",
        "        for _ in range(num_modifications):\n",
        "            anomaly_type = np.random.choice([\n",
        "                'sudden_altitude_change',\n",
        "                'impossible_speed',\n",
        "                'erratic_heading',\n",
        "                'position_jump',\n",
        "                'vertical_rate_anomaly',\n",
        "                'inconsistent_values'\n",
        "            ])\n",
        "\n",
        "            if anomaly_type == 'sudden_altitude_change':\n",
        "                anomaly['alt'] += np.random.choice([-15000, 15000])\n",
        "                anomaly['vertRate'] = np.random.choice([-8000, 8000])\n",
        "\n",
        "            elif anomaly_type == 'impossible_speed':\n",
        "                anomaly['gs'] = np.random.uniform(700, 1000)\n",
        "\n",
        "            elif anomaly_type == 'erratic_heading':\n",
        "                anomaly['heading'] = (float(base_flight['heading']) + np.random.uniform(150, 180)) % 360\n",
        "\n",
        "            elif anomaly_type == 'position_jump':\n",
        "                anomaly['lat'] += np.random.uniform(-5, 5)\n",
        "                anomaly['lon'] += np.random.uniform(-5, 5)\n",
        "\n",
        "            elif anomaly_type == 'vertical_rate_anomaly':\n",
        "                anomaly['vertRate'] = np.random.choice([-12000, 12000])\n",
        "\n",
        "            elif anomaly_type == 'inconsistent_values':\n",
        "                if np.random.random() > 0.5:\n",
        "                    # High altitude but low ground speed\n",
        "                    anomaly['alt'] = 40000\n",
        "                    anomaly['gs'] = 100\n",
        "                else:\n",
        "                    # Low altitude but very high ground speed\n",
        "                    anomaly['alt'] = 1000\n",
        "                    anomaly['gs'] = 600\n",
        "\n",
        "        anomalies.append(anomaly)\n",
        "\n",
        "    anomaly_df = pd.DataFrame(anomalies)\n",
        "    print(f\"\\nCreated {num_anomalies} synthetic anomalies\")\n",
        "    return anomaly_df\n",
        "\n",
        "class AnomalyDetector:\n",
        "    def __init__(self, model, scaler, threshold_multiplier=2.0):\n",
        "        self.model = model\n",
        "        self.scaler = scaler\n",
        "        self.threshold = None\n",
        "        self.threshold_multiplier = threshold_multiplier\n",
        "\n",
        "    def fit_threshold(self, normal_data):\n",
        "        \"\"\"Determine threshold using normal data\"\"\"\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            normal_tensor = torch.FloatTensor(normal_data)\n",
        "            predictions = self.model(normal_tensor)\n",
        "            reconstruction_errors = torch.mean((normal_tensor - predictions) ** 2, dim=1)\n",
        "\n",
        "            # Use mean + std for threshold\n",
        "            self.threshold = (torch.mean(reconstruction_errors) +\n",
        "                            self.threshold_multiplier * torch.std(reconstruction_errors))\n",
        "            print(f\"\\nSet anomaly threshold at: {self.threshold:.6f}\")\n",
        "\n",
        "    def predict(self, data, return_scores=False):\n",
        "        \"\"\"Predict anomalies in new data\"\"\"\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            data_tensor = torch.FloatTensor(data)\n",
        "            predictions = self.model(data_tensor)\n",
        "            reconstruction_errors = torch.mean((data_tensor - predictions) ** 2, dim=1)\n",
        "\n",
        "            anomalies = reconstruction_errors > self.threshold\n",
        "\n",
        "            if return_scores:\n",
        "                return anomalies.numpy(), reconstruction_errors.numpy()\n",
        "            return anomalies.numpy()\n",
        "\n",
        "    def evaluate(self, normal_data, anomaly_data):\n",
        "        \"\"\"Evaluate detector performance\"\"\"\n",
        "        normal_predictions, normal_scores = self.predict(normal_data, return_scores=True)\n",
        "        false_positives = np.sum(normal_predictions)\n",
        "        false_positive_rate = (false_positives / len(normal_data)) * 100\n",
        "\n",
        "        anomaly_predictions, anomaly_scores = self.predict(anomaly_data, return_scores=True)\n",
        "        detected_anomalies = np.sum(anomaly_predictions)\n",
        "        detection_rate = (detected_anomalies / len(anomaly_data)) * 100\n",
        "\n",
        "        print(\"\\nAnomaly Detection Performance:\")\n",
        "        print(f\"False Positives: {false_positives} out of {len(normal_data)} normal points ({false_positive_rate:.2f}%)\")\n",
        "        print(f\"True Positives: {detected_anomalies} out of {len(anomaly_data)} anomalies ({detection_rate:.2f}%)\")\n",
        "        print(\"\\nReconstruction Error Statistics:\")\n",
        "        print(f\"Normal data - Mean: {np.mean(normal_scores):.6f}, Std: {np.std(normal_scores):.6f}\")\n",
        "        print(f\"Anomaly data - Mean: {np.mean(anomaly_scores):.6f}, Std: {np.std(anomaly_scores):.6f}\")\n",
        "\n",
        "        return {\n",
        "            'false_positive_rate': false_positive_rate,\n",
        "            'detection_rate': detection_rate,\n",
        "            'normal_scores': normal_scores,\n",
        "            'anomaly_scores': anomaly_scores\n",
        "        }\n",
        "\n",
        "def train_model(model, train_data, val_data, epochs=100, batch_size=64):\n",
        "    \"\"\"Train model with improved training loop\"\"\"\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
        "\n",
        "    train_tensor = torch.FloatTensor(train_data)\n",
        "    val_tensor = torch.FloatTensor(val_data)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "    patience = 20\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for i in range(0, len(train_tensor), batch_size):\n",
        "            batch = train_tensor[i:i+batch_size]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch)\n",
        "            loss = criterion(outputs, batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(val_tensor)\n",
        "            val_loss = criterion(val_outputs, val_tensor)\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_state = model.state_dict().copy()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss/(len(train_tensor)//batch_size):.6f}, Val Loss: {val_loss:.6f}')\n",
        "\n",
        "    # Restore best model\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    return model\n",
        "\n",
        "def save_model(model, scaler, detector, save_dir='saved_models'):\n",
        "    \"\"\"Save the trained model, scaler, and detector configuration\"\"\"\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    model_path = os.path.join(save_dir, 'flight_anomaly_model.pth')\n",
        "    scaler_path = os.path.join(save_dir, 'scaler.joblib')\n",
        "    detector_path = os.path.join(save_dir, 'detector.joblib')\n",
        "\n",
        "    # Save the PyTorch model\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'input_size': model.network[0].in_features\n",
        "    }, model_path)\n",
        "\n",
        "    # Save the scaler\n",
        "    joblib.dump(scaler, scaler_path)\n",
        "\n",
        "    # Save detector configuration\n",
        "    detector_config = {\n",
        "        'threshold': detector.threshold.item() if detector.threshold is not None else None,\n",
        "        'threshold_multiplier': detector.threshold_multiplier\n",
        "    }\n",
        "    joblib.dump(detector_config, detector_path)\n",
        "\n",
        "    print(f\"\\nModel saved successfully\")\n",
        "\n",
        "def load_model(model_path='saved_models/flight_anomaly_model.pth',\n",
        "              scaler_path='saved_models/scaler.joblib',\n",
        "              detector_path='saved_models/detector.joblib'):\n",
        "    \"\"\"Load the saved model, scaler, and detector configuration\"\"\"\n",
        "    # Load model\n",
        "    checkpoint = torch.load(model_path)\n",
        "    model = FlightPredictionMLP(input_size=checkpoint['input_size'])\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    # Load scaler\n",
        "    scaler = joblib.load(scaler_path)\n",
        "\n",
        "    # Load detector configuration\n",
        "    detector_config = joblib.load(detector_path)\n",
        "    detector = AnomalyDetector(model, scaler, detector_config['threshold_multiplier'])\n",
        "    detector.threshold = torch.tensor(detector_config['threshold'])\n",
        "\n",
        "    print(f\"\\nModel loaded successfully\")\n",
        "    return model, scaler, detector\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # 1. Load and preprocess normal data\n",
        "        print(\"Loading and preprocessing data...\")\n",
        "        df = minimal_preprocess('/content/drive/MyDrive/data_MVP.json')\n",
        "\n",
        "        # 2. Prepare features\n",
        "        features = ['alt', 'gs', 'heading', 'lat', 'lon', 'vertRate', 'altChange_encoded']\n",
        "        X = df[features].values\n",
        "\n",
        "        # 3. Scale the features\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "        # 4. Split into train/validation sets\n",
        "        X_train, X_val = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "        # 5. Create and train model\n",
        "        print(\"\\nTraining model...\")\n",
        "        model = FlightPredictionMLP(input_size=len(features))\n",
        "        model = train_model(model, X_train, X_val)\n",
        "\n",
        "        # 6. Generate synthetic anomalies\n",
        "        print(\"\\nGenerating synthetic anomalies...\")\n",
        "        anomaly_df = create_synthetic_anomalies(df)\n",
        "        anomaly_features = scaler.transform(anomaly_df[features].values)\n",
        "\n",
        "        # 7. Create and evaluate anomaly detector\n",
        "        print(\"\\nEvaluating anomaly detection...\")\n",
        "        detector = AnomalyDetector(model, scaler)\n",
        "        detector.fit_threshold(X_train)\n",
        "\n",
        "        results = detector.evaluate(X_val, anomaly_features)\n",
        "\n",
        "        # 8. Optional: Detect anomalies in sample data\n",
        "        print(\"\\nExample: Detecting anomalies in sample data...\")\n",
        "        sample_data = X_val[:5]\n",
        "        predictions = detector.predict(sample_data)\n",
        "        for i, is_anomaly in enumerate(predictions):\n",
        "            print(f\"Sample {i+1}: {'Anomaly' if is_anomaly else 'Normal'}\")\n",
        "\n",
        "        # 9. Save model and components\n",
        "        print(\"\\nSaving model...\")\n",
        "        save_model(model, scaler, detector)\n",
        "\n",
        "        # model, scaler, detector = load_model()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main execution: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import json\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "class EnhancedFlightPredictionMLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes=[256, 128, 64, 32, 64, 128, 256]):\n",
        "        \"\"\"\n",
        "        Enhanced autoencoder with larger capacity and more sophisticated architecture\n",
        "\n",
        "        Args:\n",
        "            input_size: Number of input features\n",
        "            hidden_sizes: List of hidden layer sizes for the encoder and decoder\n",
        "        \"\"\"\n",
        "        super(EnhancedFlightPredictionMLP, self).__init__()\n",
        "\n",
        "        # Ensure symmetric architecture for proper autoencoder\n",
        "        self.input_size = input_size\n",
        "\n",
        "        # Build encoder layers\n",
        "        encoder_layers = []\n",
        "        prev_size = input_size\n",
        "        for h_size in hidden_sizes[:len(hidden_sizes)//2 + 1]:\n",
        "            encoder_layers.append(nn.Linear(prev_size, h_size))\n",
        "            encoder_layers.append(nn.BatchNorm1d(h_size))\n",
        "            encoder_layers.append(nn.LeakyReLU(0.2))\n",
        "            encoder_layers.append(nn.Dropout(0.25))\n",
        "            prev_size = h_size\n",
        "\n",
        "        self.encoder = nn.Sequential(*encoder_layers)\n",
        "\n",
        "        # Build decoder layers (reverse of encoder)\n",
        "        decoder_layers = []\n",
        "        for h_size in hidden_sizes[len(hidden_sizes)//2 + 1:]:\n",
        "            decoder_layers.append(nn.Linear(prev_size, h_size))\n",
        "            decoder_layers.append(nn.BatchNorm1d(h_size))\n",
        "            decoder_layers.append(nn.LeakyReLU(0.2))\n",
        "            decoder_layers.append(nn.Dropout(0.25))\n",
        "            prev_size = h_size\n",
        "\n",
        "        # Final layer to reconstruct input\n",
        "        decoder_layers.append(nn.Linear(prev_size, input_size))\n",
        "\n",
        "        self.decoder = nn.Sequential(*decoder_layers)\n",
        "\n",
        "        # Initialize weights for better gradient flow\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"Initialize weights using He initialization for better training\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through the autoencoder\"\"\"\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\"Get encoded representation\"\"\"\n",
        "        return self.encoder(x)\n",
        "\n",
        "def minimal_preprocess(filepath: str):\n",
        "    \"\"\"Minimal preprocessing - only handle format issues\"\"\"\n",
        "    with open(filepath, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    initial_count = len(df)\n",
        "    print(f\"\\nInitial entries: {initial_count}\")\n",
        "\n",
        "    features = ['alt', 'gs', 'heading', 'lat', 'lon', 'vertRate']\n",
        "\n",
        "    # Convert to numeric, keeping all values\n",
        "    for col in features:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "            nan_count = df[col].isna().sum()\n",
        "            if nan_count > 0:\n",
        "                print(f\"Found {nan_count} missing/invalid values in {col}\")\n",
        "                median_val = df[col].median()\n",
        "                df[col] = df[col].fillna(median_val)\n",
        "                print(f\"Filled missing values with median: {median_val}\")\n",
        "\n",
        "    # Handle altChange encoding\n",
        "    df['altChange'] = df['altChange'].fillna(0)\n",
        "    df['altChange_encoded'] = df['altChange'].map({' ': 0, 'C': 1, 'D': -1}).fillna(0)\n",
        "\n",
        "    # Ensure time column exists\n",
        "    if 'pitr' in df.columns and 'gs' in df.columns and len(df) > 1:\n",
        "        df = df.sort_values(['id', 'pitr']).reset_index(drop=True)\n",
        "\n",
        "        # Compute time difference in seconds\n",
        "        df['time_diff'] = df.groupby('id')['pitr'].diff().fillna(1)  # Avoid division by zero\n",
        "\n",
        "        # Compute rate of change (change per second)\n",
        "        df['gs_change_rate'] = df.groupby('id')['gs'].diff() / df['time_diff']\n",
        "        df['heading_change_rate'] = df.groupby('id')['heading'].diff().apply(lambda x: (x + 180) % 360 - 180) / df['time_diff']\n",
        "\n",
        "        # Replace infinity or extreme values with 0 or NaN\n",
        "        df['gs_change_rate'] = df['gs_change_rate'].replace([np.inf, -np.inf], 0)  # Replace infinity with 0\n",
        "        df['heading_change_rate'] = df['heading_change_rate'].replace([np.inf, -np.inf], 0)  # Replace infinity with 0\n",
        "\n",
        "        # Handle NaN values (if any remain after the replacement) by filling with 0\n",
        "        df['gs_change_rate'] = df['gs_change_rate'].fillna(0)\n",
        "        df['heading_change_rate'] = df['heading_change_rate'].fillna(0)\n",
        "\n",
        "    print(f\"\\nFeature engineering complete\")\n",
        "    print(df)\n",
        "\n",
        "    print(f\"\\nFinal entries: {len(df)}\")\n",
        "    return df\n",
        "\n",
        "def create_synthetic_anomalies(df, num_anomalies=1000):\n",
        "    \"\"\"Generate synthetic anomalies focusing on commercial flight patterns\"\"\"\n",
        "    anomalies = []\n",
        "\n",
        "    # Define commercial flight characteristics\n",
        "    COMMERCIAL_MIN_CRUISE_SPEED = 450  # in knots\n",
        "    COMMERCIAL_MAX_CRUISE_SPEED = 550  # in knots\n",
        "    COMMERCIAL_MIN_CRUISE_ALT = 30000  # in feet\n",
        "    COMMERCIAL_MAX_CRUISE_ALT = 40000  # in feet\n",
        "    COMMERCIAL_NORMAL_VERT_RATE = 2000  # in feet/min\n",
        "    COMMERCIAL_NORMAL_TURN_RATE = 3  # in degrees/second\n",
        "\n",
        "    # Define anomaly types with probabilities\n",
        "    anomaly_types = [\n",
        "        # Speed anomalies (40% chance)\n",
        "        {'type': 'high_speed', 'prob': 0.15},\n",
        "        {'type': 'low_speed_high_alt', 'prob': 0.15},\n",
        "        {'type': 'sudden_speed_change', 'prob': 0.10},\n",
        "\n",
        "        # Altitude anomalies (30% chance)\n",
        "        {'type': 'excessive_altitude', 'prob': 0.10},\n",
        "        {'type': 'low_altitude_high_speed', 'prob': 0.10},\n",
        "        {'type': 'rapid_altitude_drop', 'prob': 0.10},\n",
        "\n",
        "        # Heading anomalies (15% chance)\n",
        "        {'type': 'impossible_turn', 'prob': 0.05},\n",
        "        {'type': 'erratic_heading', 'prob': 0.10},\n",
        "\n",
        "        # Course & pattern anomalies (15% chance)\n",
        "        {'type': 'course_deviation', 'prob': 0.05},\n",
        "        {'type': 'unusual_loitering', 'prob': 0.05},\n",
        "        {'type': 'altitude_speed_fluctuations', 'prob': 0.05}\n",
        "    ]\n",
        "\n",
        "    # Calculate cumulative probabilities\n",
        "    cum_probs = np.cumsum([a['prob'] for a in anomaly_types])\n",
        "\n",
        "    for _ in range(num_anomalies):\n",
        "        # Sample a base flight\n",
        "        base_flight = df.sample(n=1).iloc[0].to_dict()\n",
        "        anomaly = base_flight.copy()\n",
        "\n",
        "        # Randomly select an anomaly type based on probabilities\n",
        "        rand_val = np.random.random()\n",
        "        anomaly_idx = np.searchsorted(cum_probs, rand_val)\n",
        "        anomaly_type = anomaly_types[anomaly_idx]['type']\n",
        "\n",
        "        # Apply the selected anomaly\n",
        "        if anomaly_type == 'high_speed':\n",
        "            anomaly['anomaly_type'] = anomaly_type\n",
        "            # Commercial aircraft flying faster than 600 knots\n",
        "            anomaly['gs'] = np.random.uniform(600, 1000)\n",
        "            #anomaly['gs_change'] = np.random.uniform(50, 100)  # Accelerating\n",
        "            anomalies.append(anomaly)\n",
        "\n",
        "        elif anomaly_type == 'low_speed_high_alt':\n",
        "            anomaly['anomaly_type'] = anomaly_type\n",
        "            # Aircraft flying below 200 knots at high altitude (stall risk)\n",
        "            anomaly['gs'] = np.random.uniform(100, 200)\n",
        "            anomaly['alt'] = np.random.uniform(35000, 40000)\n",
        "            #anomaly['gs_change'] = np.random.uniform(-50, -20)  # Decelerating\n",
        "            anomalies.append(anomaly)\n",
        "\n",
        "        elif anomaly_type == 'sudden_speed_change':\n",
        "            anomaly['anomaly_type'] = anomaly_type\n",
        "            # Sudden ±200 knots change (not physically possible)\n",
        "            direction = np.random.choice([-1, 1])\n",
        "            speed_change = np.random.uniform(200, 250) * direction\n",
        "\n",
        "            anomaly['gs'] = max(0, base_flight['gs'] + speed_change)  # Ensure non-negative speed\n",
        "            time_diff = np.random.uniform(1, 3)\n",
        "            # Ensure 'time_diff' exists and is valid\n",
        "            anomaly['gs_change_rate'] = speed_change / time_diff\n",
        "            anomalies.append(anomaly)\n",
        "\n",
        "\n",
        "        elif anomaly_type == 'excessive_altitude':\n",
        "            anomaly['anomaly_type'] = anomaly_type\n",
        "            # Commercial aircraft flying above 45,000 ft\n",
        "            anomaly['alt'] = np.random.uniform(45000, 60000)\n",
        "            #anomaly['vertRate'] = np.random.uniform(1000, 2000)  # Still climbing\n",
        "            anomalies.append(anomaly)\n",
        "\n",
        "        elif anomaly_type == 'low_altitude_high_speed':\n",
        "            anomaly['anomaly_type'] = anomaly_type\n",
        "            # High speed at low altitude\n",
        "            anomaly['alt'] = np.random.uniform(5000, 10000)\n",
        "            anomaly['gs'] = np.random.uniform(450, 550)\n",
        "            #anomaly['vertRate'] = np.random.uniform(-1000, 1000)  # Level or slight descent/climb\n",
        "            anomalies.append(anomaly)\n",
        "\n",
        "        elif anomaly_type == 'rapid_altitude_drop':\n",
        "            anomaly['anomaly_type'] = anomaly_type\n",
        "            # Rapid altitude drop (possible emergency)\n",
        "            anomaly['vertRate'] = np.random.uniform(-8000, -5000)\n",
        "            #anomaly['gs_change'] = np.random.uniform(-50, 50)  # Possible speed changes during emergency\n",
        "            anomalies.append(anomaly)\n",
        "\n",
        "        elif anomaly_type == 'impossible_turn':\n",
        "            anomaly['anomaly_type'] = anomaly_type\n",
        "            # Physically impossible turns for aircraft\n",
        "            direction = np.random.choice([-1, 1])  # Left (-1) or right (+1)\n",
        "            heading_change = direction * np.random.uniform(90, 120)  # Extreme turn rate\n",
        "\n",
        "            anomaly['heading'] = (base_flight['heading'] + heading_change) % 360\n",
        "            # Use a random time difference (1 to 5 seconds) to ensure a reasonable heading change rate\n",
        "            time_diff = np.random.uniform(1, 5)  # Time in seconds\n",
        "            anomaly['heading_change_rate'] = heading_change / time_diff  # Degrees per second\n",
        "            anomalies.append(anomaly)\n",
        "\n",
        "        elif anomaly_type == 'erratic_heading':\n",
        "            anomaly['anomaly_type'] = anomaly_type\n",
        "            # Create zigzag pattern in heading\n",
        "            direction = np.random.choice([-1, 1])  # Left (-1) or right (+1)\n",
        "            heading_change = direction * np.random.uniform(40, 80)  # Random change in range\n",
        "\n",
        "            anomaly['heading'] = (base_flight['heading'] + heading_change) % 360\n",
        "            time_diff = np.random.uniform(1, 2)\n",
        "            anomaly['heading_change_rate'] = heading_change / time_diff\n",
        "            anomalies.append(anomaly)\n",
        "\n",
        "        elif anomaly_type == 'course_deviation':\n",
        "            anomaly['anomaly_type'] = anomaly_type\n",
        "            # Significant deviation from expected route\n",
        "            deviation_magnitude = np.random.uniform(0.3, 0.7)  # roughly 20-40 NM\n",
        "            anomaly['lat'] = float(base_flight['lat']) + deviation_magnitude * np.random.choice([-1, 1])\n",
        "            anomaly['lon'] = float(base_flight['lon']) + deviation_magnitude * np.random.choice([-1, 1])\n",
        "\n",
        "            time_diff = np.random.uniform(1, 2)  # Time in seconds\n",
        "            heading_change = np.random.uniform(20, 40)  # Course correction\n",
        "            anomaly['heading_change_rate'] = heading_change / time_diff  # Degrees per second\n",
        "            anomalies.append(anomaly)\n",
        "\n",
        "        elif anomaly_type == 'unusual_loitering':\n",
        "            anomaly['anomaly_type'] = anomaly_type\n",
        "            # Aircraft circling or loitering unexpectedly\n",
        "            anomaly['gs'] = np.random.uniform(50, 100)\n",
        "            anomaly['vertRate'] = np.random.uniform(-100, 100)\n",
        "\n",
        "            time_diff = np.random.uniform(1, 2)  # Time in seconds\n",
        "            heading_change = np.random.uniform(5, 15)  # Slight turning\n",
        "            anomaly['heading_change_rate'] = heading_change / time_diff  # Degrees per second\n",
        "            anomalies.append(anomaly)\n",
        "\n",
        "        elif anomaly_type == 'altitude_speed_fluctuations':\n",
        "            time_diff = np.random.uniform(1, 5)  # Time in seconds\n",
        "            if np.random.random() > 0.5:\n",
        "                anomaly['anomaly_type'] = anomaly_type\n",
        "                # Climbing rapidly while speeding up\n",
        "                anomaly['vertRate'] = np.random.uniform(3000, 4000)\n",
        "                speed_change = np.random.uniform(50, 100)\n",
        "                anomaly['gs'] = float(base_flight['gs']) + speed_change\n",
        "                anomaly['gs_change_rate'] = speed_change / time_diff  # Knots per second\n",
        "            else:\n",
        "                # Descending rapidly while slowing down\n",
        "                anomaly['vertRate'] = np.random.uniform(-4000, -3000)\n",
        "                speed_change = np.random.uniform(50, 100)\n",
        "                anomaly['gs'] = max(0, float(base_flight['gs']) - speed_change)  # Ensure non-negative speed\n",
        "                anomaly['gs_change_rate'] = -speed_change / time_diff  # Knots per second (negative for slowing down)\n",
        "\n",
        "                # Add label for what type of anomaly this is (for analysis)\n",
        "                anomaly['anomaly_type'] = anomaly_type\n",
        "            anomalies.append(anomaly)\n",
        "\n",
        "\n",
        "    anomaly_df = pd.DataFrame(anomalies)\n",
        "\n",
        "    # Print statistics about the generated anomalies\n",
        "    anomaly_counts = anomaly_df['anomaly_type'].value_counts()\n",
        "    print(\"\\nSynthetic Commercial Flight Anomalies Created:\")\n",
        "    for anomaly_type, count in anomaly_counts.items():\n",
        "        print(f\"  - {anomaly_type}: {count} instances ({count/num_anomalies*100:.1f}%)\")\n",
        "\n",
        "    return anomaly_df\n",
        "\n",
        "class AnomalyDetector:\n",
        "    def __init__(self, model, scaler, threshold_multiplier=2.5):\n",
        "        self.model = model\n",
        "        self.scaler = scaler\n",
        "        self.threshold = None\n",
        "        self.threshold_multiplier = threshold_multiplier\n",
        "\n",
        "    def fit_threshold(self, normal_data):\n",
        "        \"\"\"Determine threshold using normal data with enhanced approach\"\"\"\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            normal_tensor = torch.FloatTensor(normal_data)\n",
        "            predictions = self.model(normal_tensor)\n",
        "            reconstruction_errors = torch.mean((normal_tensor - predictions) ** 2, dim=1)\n",
        "\n",
        "            # Use more sophisticated threshold calculation\n",
        "            # Based on percentile rather than just mean + std\n",
        "            sorted_errors = torch.sort(reconstruction_errors)[0]\n",
        "            percentile_95 = sorted_errors[int(0.95 * len(sorted_errors))]\n",
        "            percentile_99 = sorted_errors[int(0.99 * len(sorted_errors))]\n",
        "\n",
        "            # Use a weighted combination of statistics\n",
        "            self.threshold = 0.5 * percentile_95 + 0.5 * percentile_99\n",
        "\n",
        "            print(f\"\\nSet anomaly threshold at: {self.threshold:.6f}\")\n",
        "            print(f\"95th percentile: {percentile_95:.6f}\")\n",
        "            print(f\"99th percentile: {percentile_99:.6f}\")\n",
        "\n",
        "    def predict(self, data, return_scores=False):\n",
        "        \"\"\"Predict anomalies in new data\"\"\"\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            data_tensor = torch.FloatTensor(data)\n",
        "            predictions = self.model(data_tensor)\n",
        "            reconstruction_errors = torch.mean((data_tensor - predictions) ** 2, dim=1)\n",
        "\n",
        "            anomalies = reconstruction_errors > self.threshold\n",
        "\n",
        "            if return_scores:\n",
        "                return anomalies.numpy(), reconstruction_errors.numpy()\n",
        "            return anomalies.numpy()\n",
        "\n",
        "    def evaluate(self, normal_data, anomaly_data):\n",
        "        \"\"\"Evaluate detector performance\"\"\"\n",
        "        normal_predictions, normal_scores = self.predict(normal_data, return_scores=True)\n",
        "        false_positives = np.sum(normal_predictions)\n",
        "        false_positive_rate = (false_positives / len(normal_data)) * 100\n",
        "\n",
        "        anomaly_predictions, anomaly_scores = self.predict(anomaly_data, return_scores=True)\n",
        "        detected_anomalies = np.sum(anomaly_predictions)\n",
        "        detection_rate = (detected_anomalies / len(anomaly_data)) * 100\n",
        "\n",
        "        print(\"\\nAnomaly Detection Performance:\")\n",
        "        print(f\"False Positives: {false_positives} out of {len(normal_data)} normal points ({false_positive_rate:.2f}%)\")\n",
        "        print(f\"True Positives: {detected_anomalies} out of {len(anomaly_data)} anomalies ({detection_rate:.2f}%)\")\n",
        "        print(\"\\nReconstruction Error Statistics:\")\n",
        "        print(f\"Normal data - Mean: {np.mean(normal_scores):.6f}, Std: {np.std(normal_scores):.6f}\")\n",
        "        print(f\"Anomaly data - Mean: {np.mean(anomaly_scores):.6f}, Std: {np.std(anomaly_scores):.6f}\")\n",
        "\n",
        "        # Analyze performance by anomaly type\n",
        "        if 'anomaly_type' in anomaly_data:\n",
        "            print(\"\\nDetection Rate by Anomaly Type:\")\n",
        "            anomaly_df = pd.DataFrame({\n",
        "                'anomaly_type': anomaly_data['anomaly_type'],\n",
        "                'is_detected': anomaly_predictions,\n",
        "                'score': anomaly_scores\n",
        "            })\n",
        "\n",
        "            for anomaly_type, group in anomaly_df.groupby('anomaly_type'):\n",
        "                detection_rate = (group['is_detected'].sum() / len(group)) * 100\n",
        "                avg_score = group['score'].mean()\n",
        "                print(f\"  - {anomaly_type}: {detection_rate:.2f}% detected (avg score: {avg_score:.6f})\")\n",
        "\n",
        "        return {\n",
        "            'false_positive_rate': false_positive_rate,\n",
        "            'detection_rate': detection_rate,\n",
        "            'normal_scores': normal_scores,\n",
        "            'anomaly_scores': anomaly_scores\n",
        "        }\n",
        "\n",
        "def train_model(model, train_data, val_data, epochs=150, batch_size=128):\n",
        "    \"\"\"Train model with improved training loop and learning rate scheduling\"\"\"\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "    # Multi-step learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(\n",
        "        optimizer,\n",
        "        milestones=[30, 60, 90, 120],\n",
        "        gamma=0.5\n",
        "    )\n",
        "\n",
        "    train_tensor = torch.FloatTensor(train_data)\n",
        "    val_tensor = torch.FloatTensor(val_data)\n",
        "\n",
        "    # For early stopping\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "    patience = 25  # Increased patience for larger model\n",
        "    patience_counter = 0\n",
        "\n",
        "    # Training history\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        batch_count = 0\n",
        "\n",
        "        # Create random permutation for batch sampling\n",
        "        indices = torch.randperm(len(train_tensor))\n",
        "\n",
        "        for i in range(0, len(train_tensor), batch_size):\n",
        "            # Get batch using permutation\n",
        "            batch_indices = indices[i:i+batch_size]\n",
        "            batch = train_tensor[batch_indices]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch)\n",
        "            loss = criterion(outputs, batch)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            batch_count += 1\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(val_tensor)\n",
        "            val_loss = criterion(val_outputs, val_tensor)\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step()\n",
        "\n",
        "        # Save losses for plotting\n",
        "        avg_train_loss = total_loss / batch_count\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(val_loss.item())\n",
        "\n",
        "        # Early stopping check\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_state = model.state_dict().copy()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.6f}, Val Loss: {val_loss:.6f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n",
        "\n",
        "    # Restore best model\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    return model, train_losses, val_losses\n",
        "\n",
        "def save_model(model, scaler, detector, save_dir='saved_models'):\n",
        "    \"\"\"Save the trained model, scaler, and detector configuration\"\"\"\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    model_path = os.path.join(save_dir, 'flight_anomaly_model.pth')\n",
        "    scaler_path = os.path.join(save_dir, 'scaler.joblib')\n",
        "    detector_path = os.path.join(save_dir, 'detector.joblib')\n",
        "\n",
        "    # Save the PyTorch model\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'input_size': model.input_size,\n",
        "        'model_type': 'EnhancedFlightPredictionMLP'\n",
        "    }, model_path)\n",
        "\n",
        "    # Save the scaler\n",
        "    joblib.dump(scaler, scaler_path)\n",
        "\n",
        "    # Save detector configuration\n",
        "    detector_config = {\n",
        "        'threshold': detector.threshold.item() if detector.threshold is not None else None,\n",
        "        'threshold_multiplier': detector.threshold_multiplier\n",
        "    }\n",
        "    joblib.dump(detector_config, detector_path)\n",
        "\n",
        "    print(f\"\\nModel saved successfully\")\n",
        "\n",
        "def load_model(model_path='saved_models/flight_anomaly_model.pth',\n",
        "              scaler_path='saved_models/scaler.joblib',\n",
        "              detector_path='saved_models/detector.joblib'):\n",
        "    \"\"\"Load the saved model, scaler, and detector configuration\"\"\"\n",
        "    # Load model\n",
        "    checkpoint = torch.load(model_path)\n",
        "    model_type = checkpoint.get('model_type', 'FlightPredictionMLP')\n",
        "\n",
        "    if model_type == 'EnhancedFlightPredictionMLP':\n",
        "        model = EnhancedFlightPredictionMLP(input_size=checkpoint['input_size'])\n",
        "    else:\n",
        "        # Fallback for compatibility with older models\n",
        "        from collections import OrderedDict\n",
        "        model = FlightPredictionMLP(input_size=checkpoint['input_size'])\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    # Load scaler\n",
        "    scaler = joblib.load(scaler_path)\n",
        "\n",
        "    # Load detector configuration\n",
        "    detector_config = joblib.load(detector_path)\n",
        "    detector = AnomalyDetector(model, scaler, detector_config['threshold_multiplier'])\n",
        "    detector.threshold = torch.tensor(detector_config['threshold'])\n",
        "\n",
        "    print(f\"\\nModel loaded successfully: {model_type}\")\n",
        "    return model, scaler, detector\n",
        "\n",
        "def analyze_anomaly_scores(anomaly_df, scores, detector_threshold):\n",
        "    \"\"\"Analyze detection performance for different types of anomalies\"\"\"\n",
        "    anomaly_df['score'] = scores\n",
        "    anomaly_df['is_detected'] = scores > detector_threshold\n",
        "\n",
        "    print(\"\\nAnomaly Detection Analysis by Type:\")\n",
        "    for anomaly_type, group in anomaly_df.groupby('anomaly_type'):\n",
        "        detection_rate = (group['is_detected'].sum() / len(group)) * 100\n",
        "        avg_score = group['score'].mean()\n",
        "        max_score = group['score'].max()\n",
        "        min_score = group['score'].min()\n",
        "\n",
        "        print(f\"\\n{anomaly_type.upper()}:\")\n",
        "        print(f\"  - Detection rate: {detection_rate:.2f}%\")\n",
        "        print(f\"  - Average score: {avg_score:.6f}\")\n",
        "        print(f\"  - Score range: {min_score:.6f} to {max_score:.6f}\")\n",
        "\n",
        "        # Find hardest to detect examples\n",
        "        if not group['is_detected'].all() and len(group) > 1:\n",
        "            missed = group[~group['is_detected']].sort_values('score', ascending=True)\n",
        "            if len(missed) > 0:\n",
        "                print(f\"  - Hardest to detect example (score: {missed.iloc[0]['score']:.6f}):\")\n",
        "                for feature in ['alt', 'gs', 'heading', 'vertRate','gs_change_rate','heading_change_rate']:\n",
        "                    if feature in missed.columns:\n",
        "                        print(f\"    {feature}: {missed.iloc[0][feature]}\")\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # 1. Load and preprocess normal data\n",
        "        print(\"Loading and preprocessing data...\")\n",
        "        df = minimal_preprocess('/content/drive/MyDrive/data_MVP.json')\n",
        "        # 2. Prepare features\n",
        "        features = ['alt', 'gs', 'heading', 'lat', 'lon', 'vertRate', 'altChange_encoded']\n",
        "        # Add new engineered features if they exist\n",
        "        if 'gs_change_rate' in df.columns:\n",
        "            features.append('gs_change_rate')\n",
        "        if 'heading_change_rate' in df.columns:\n",
        "            features.append('heading_change_rate')\n",
        "\n",
        "        X = df[features].values\n",
        "\n",
        "        # 3. Scale the features\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "        # 4. Split into train/validation sets\n",
        "        X_train, X_val = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "        # 5. Create and train the enhanced model\n",
        "        print(\"\\nTraining enhanced model with larger architecture...\")\n",
        "        model = EnhancedFlightPredictionMLP(input_size=len(features))\n",
        "        model, train_losses, val_losses = train_model(model, X_train, X_val, epochs=150, batch_size=128)\n",
        "        #model, scaler, detector = load_model()\n",
        "        # 6. Generate synthetic commercial flight anomalies\n",
        "        print(\"\\nGenerating synthetic commercial flight anomalies...\")\n",
        "        anomaly_df = create_synthetic_anomalies(df, num_anomalies=1000)  # Increased number of anomalies\n",
        "\n",
        "        # Create a dataframe for analysis that includes the anomaly type\n",
        "        anomaly_types = anomaly_df['anomaly_type'].copy()\n",
        "        print(anomaly_types)\n",
        "        # Extract only the features for anomaly detection\n",
        "        anomaly_features = anomaly_df[features].values\n",
        "        anomaly_features_scaled = scaler.transform(anomaly_features)\n",
        "\n",
        "        # 7. Create and evaluate anomaly detector with adjusted threshold\n",
        "        print(\"\\nEvaluating commercial flight anomaly detection...\")\n",
        "        detector = AnomalyDetector(model, scaler, threshold_multiplier=2.5)  # Adjusted threshold\n",
        "        detector.fit_threshold(X_train)\n",
        "\n",
        "        # Basic evaluation\n",
        "        results = detector.evaluate(X_val, anomaly_features_scaled)\n",
        "\n",
        "        # 8. Detailed analysis by anomaly type\n",
        "        print(\"\\nPerforming detailed analysis by anomaly type...\")\n",
        "        with torch.no_grad():\n",
        "            anomaly_tensor = torch.FloatTensor(anomaly_features_scaled)\n",
        "            predictions = model(anomaly_tensor)\n",
        "            reconstruction_errors = torch.mean((anomaly_tensor - predictions) ** 2, dim=1).numpy()\n",
        "\n",
        "        # Add the anomaly type back for analysis\n",
        "        anomaly_analysis_df = pd.DataFrame({\n",
        "            'anomaly_type': anomaly_types,\n",
        "            'score': reconstruction_errors,\n",
        "            'is_detected': reconstruction_errors > detector.threshold.item()\n",
        "        })\n",
        "\n",
        "        # Include original feature values for analysis\n",
        "        for col in features:\n",
        "            if col in anomaly_df.columns:\n",
        "                anomaly_analysis_df[col] = anomaly_df[col].values\n",
        "\n",
        "        analyze_anomaly_scores(anomaly_analysis_df, reconstruction_errors, detector.threshold.item())\n",
        "\n",
        "        # 9. Save model and components\n",
        "        print(\"\\nSaving enhanced model...\")\n",
        "        save_model(model, scaler, detector)\n",
        "\n",
        "        # Optionally load the model back to verify\n",
        "        #\n",
        "\n",
        "        print(\"\\nEnhanced model training and evaluation complete!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main execution: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPDHi5L7R6J8",
        "outputId": "64ebaa6c-415b-45a4-9d47-3f5106182ca5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preprocessing data...\n",
            "\n",
            "Initial entries: 39092\n",
            "Found 5507 missing/invalid values in alt\n",
            "Filled missing values with median: 32275.0\n",
            "Found 5491 missing/invalid values in gs\n",
            "Filled missing values with median: 416.0\n",
            "Found 3 missing/invalid values in heading\n",
            "Filled missing values with median: 160.0\n",
            "Found 1 missing/invalid values in lat\n",
            "Filled missing values with median: 25.54697\n",
            "Found 1 missing/invalid values in lon\n",
            "Filled missing values with median: 50.64428\n",
            "Found 5850 missing/invalid values in vertRate\n",
            "Filled missing values with median: 0.0\n",
            "\n",
            "Feature engineering complete\n",
            "               pitr      alt altChange                        id     gs  \\\n",
            "0      1.682017e+09    600.0            4LMWC-1682015398-adhoc-0  168.0   \n",
            "1      1.682017e+09   2025.0         C  4LMWC-1682015398-adhoc-0  212.0   \n",
            "2      1.682017e+09   2175.0         C  4LMWC-1682015398-adhoc-0  246.0   \n",
            "3      1.682017e+09   2150.0            4LMWC-1682015398-adhoc-0  256.0   \n",
            "4      1.682017e+09   2150.0            4LMWC-1682015398-adhoc-0  259.0   \n",
            "...             ...      ...       ...                       ...    ...   \n",
            "39087  1.682019e+09  37000.0            YRBEE-1682010317-adhoc-0  465.0   \n",
            "39088  1.682019e+09  37000.0            YRBEE-1682010317-adhoc-0  462.0   \n",
            "39089  1.682019e+09  36975.0            YRBEE-1682010317-adhoc-0  461.0   \n",
            "39090  1.682019e+09  36950.0            YRBEE-1682010317-adhoc-0  462.0   \n",
            "39091           NaN  32275.0         0                       NaN  416.0   \n",
            "\n",
            "       heading       lat       lon  vertRate  altChange_encoded  time_diff  \\\n",
            "0        303.0  25.33580  55.50985       0.0                0.0        1.0   \n",
            "1        302.0  25.36281  55.46254     640.0                1.0       61.0   \n",
            "2        302.0  25.37930  55.43359       0.0                1.0       27.0   \n",
            "3        302.0  25.38940  55.41597       0.0                0.0       17.0   \n",
            "4        303.0  25.40793  55.38338       0.0                0.0       29.0   \n",
            "...        ...       ...       ...       ...                ...        ...   \n",
            "39087    144.0  29.59265  31.81344       0.0                0.0       48.0   \n",
            "39088    143.0  29.53871  31.85887    -448.0                0.0       31.0   \n",
            "39089    144.0  29.46881  31.91782     -64.0                0.0       38.0   \n",
            "39090    144.0  29.41692  31.96138       0.0                0.0       39.0   \n",
            "39091    160.0  25.54697  50.64428       0.0                0.0        1.0   \n",
            "\n",
            "       gs_change_rate  heading_change_rate  \n",
            "0            0.000000             0.000000  \n",
            "1            0.721311            -0.016393  \n",
            "2            1.259259             0.000000  \n",
            "3            0.588235             0.000000  \n",
            "4            0.103448             0.034483  \n",
            "...               ...                  ...  \n",
            "39087       -0.083333             0.000000  \n",
            "39088       -0.096774            -0.032258  \n",
            "39089       -0.026316             0.026316  \n",
            "39090        0.025641             0.000000  \n",
            "39091        0.000000             0.000000  \n",
            "\n",
            "[39092 rows x 13 columns]\n",
            "\n",
            "Final entries: 39092\n",
            "\n",
            "Training enhanced model with larger architecture...\n",
            "Epoch [10/150], Train Loss: 0.240409, Val Loss: 0.077770, LR: 0.001000\n",
            "Epoch [20/150], Train Loss: 0.188826, Val Loss: 0.060706, LR: 0.001000\n",
            "Epoch [30/150], Train Loss: 0.167611, Val Loss: 0.061806, LR: 0.000500\n",
            "Epoch [40/150], Train Loss: 0.153948, Val Loss: 0.076895, LR: 0.000500\n",
            "Epoch [50/150], Train Loss: 0.149352, Val Loss: 0.056441, LR: 0.000500\n",
            "Epoch [60/150], Train Loss: 0.140863, Val Loss: 0.054447, LR: 0.000250\n",
            "Epoch [70/150], Train Loss: 0.138525, Val Loss: 0.052528, LR: 0.000250\n",
            "Epoch [80/150], Train Loss: 0.139364, Val Loss: 0.050299, LR: 0.000250\n",
            "Epoch [90/150], Train Loss: 0.140225, Val Loss: 0.050665, LR: 0.000125\n",
            "Epoch [100/150], Train Loss: 0.136703, Val Loss: 0.044551, LR: 0.000125\n",
            "Epoch [110/150], Train Loss: 0.132485, Val Loss: 0.056328, LR: 0.000125\n",
            "Epoch [120/150], Train Loss: 0.130322, Val Loss: 0.042731, LR: 0.000063\n",
            "Epoch [130/150], Train Loss: 0.132164, Val Loss: 0.055081, LR: 0.000063\n",
            "Epoch [140/150], Train Loss: 0.129486, Val Loss: 0.057265, LR: 0.000063\n",
            "Epoch [150/150], Train Loss: 0.131186, Val Loss: 0.046697, LR: 0.000063\n",
            "\n",
            "Generating synthetic commercial flight anomalies...\n",
            "\n",
            "Synthetic Commercial Flight Anomalies Created:\n",
            "  - low_speed_high_alt: 166 instances (16.6%)\n",
            "  - high_speed: 152 instances (15.2%)\n",
            "  - erratic_heading: 110 instances (11.0%)\n",
            "  - rapid_altitude_drop: 108 instances (10.8%)\n",
            "  - low_altitude_high_speed: 108 instances (10.8%)\n",
            "  - sudden_speed_change: 89 instances (8.9%)\n",
            "  - excessive_altitude: 74 instances (7.4%)\n",
            "  - altitude_speed_fluctuations: 52 instances (5.2%)\n",
            "  - unusual_loitering: 51 instances (5.1%)\n",
            "  - impossible_turn: 48 instances (4.8%)\n",
            "  - course_deviation: 42 instances (4.2%)\n",
            "0               low_speed_high_alt\n",
            "1              rapid_altitude_drop\n",
            "2              rapid_altitude_drop\n",
            "3                  erratic_heading\n",
            "4      altitude_speed_fluctuations\n",
            "                  ...             \n",
            "995        low_altitude_high_speed\n",
            "996                erratic_heading\n",
            "997             low_speed_high_alt\n",
            "998                erratic_heading\n",
            "999                erratic_heading\n",
            "Name: anomaly_type, Length: 1000, dtype: object\n",
            "\n",
            "Evaluating commercial flight anomaly detection...\n",
            "\n",
            "Set anomaly threshold at: 0.174771\n",
            "95th percentile: 0.119254\n",
            "99th percentile: 0.230289\n",
            "\n",
            "Anomaly Detection Performance:\n",
            "False Positives: 157 out of 7819 normal points (2.01%)\n",
            "True Positives: 965 out of 1000 anomalies (96.50%)\n",
            "\n",
            "Reconstruction Error Statistics:\n",
            "Normal data - Mean: 0.046697, Std: 0.435557\n",
            "Anomaly data - Mean: 24.523203, Std: 92.627228\n",
            "\n",
            "Performing detailed analysis by anomaly type...\n",
            "\n",
            "Anomaly Detection Analysis by Type:\n",
            "\n",
            "ALTITUDE_SPEED_FLUCTUATIONS:\n",
            "  - Detection rate: 100.00%\n",
            "  - Average score: 16.813160\n",
            "  - Score range: 1.301725 to 144.680664\n",
            "\n",
            "COURSE_DEVIATION:\n",
            "  - Detection rate: 100.00%\n",
            "  - Average score: 9.682303\n",
            "  - Score range: 3.270840 to 24.759333\n",
            "\n",
            "ERRATIC_HEADING:\n",
            "  - Detection rate: 100.00%\n",
            "  - Average score: 25.798286\n",
            "  - Score range: 4.038089 to 97.302071\n",
            "\n",
            "EXCESSIVE_ALTITUDE:\n",
            "  - Detection rate: 75.68%\n",
            "  - Average score: 0.411949\n",
            "  - Score range: 0.043642 to 1.590314\n",
            "  - Hardest to detect example (score: 0.043642):\n",
            "    alt: 45008.61101641691\n",
            "    gs: 448.0\n",
            "    heading: 309.0\n",
            "    vertRate: 0.0\n",
            "    gs_change_rate: -0.13793103448275862\n",
            "    heading_change_rate: 0.0\n",
            "\n",
            "HIGH_SPEED:\n",
            "  - Detection rate: 90.13%\n",
            "  - Average score: 0.877672\n",
            "  - Score range: 0.047603 to 3.268796\n",
            "  - Hardest to detect example (score: 0.047603):\n",
            "    alt: 41000.0\n",
            "    gs: 623.4273174082601\n",
            "    heading: 102.0\n",
            "    vertRate: 0.0\n",
            "    gs_change_rate: 0.0\n",
            "    heading_change_rate: 0.0\n",
            "\n",
            "IMPOSSIBLE_TURN:\n",
            "  - Detection rate: 100.00%\n",
            "  - Average score: 36.664829\n",
            "  - Score range: 4.175410 to 245.377747\n",
            "\n",
            "LOW_ALTITUDE_HIGH_SPEED:\n",
            "  - Detection rate: 98.15%\n",
            "  - Average score: 0.433233\n",
            "  - Score range: 0.151030 to 0.819432\n",
            "  - Hardest to detect example (score: 0.151030):\n",
            "    alt: 9228.206814378495\n",
            "    gs: 452.42989569262255\n",
            "    heading: 115.0\n",
            "    vertRate: 1088.0\n",
            "    gs_change_rate: 0.27586206896551724\n",
            "    heading_change_rate: 0.0\n",
            "\n",
            "LOW_SPEED_HIGH_ALT:\n",
            "  - Detection rate: 100.00%\n",
            "  - Average score: 0.660139\n",
            "  - Score range: 0.338051 to 1.127445\n",
            "\n",
            "RAPID_ALTITUDE_DROP:\n",
            "  - Detection rate: 100.00%\n",
            "  - Average score: 1.236964\n",
            "  - Score range: 0.243544 to 3.094496\n",
            "\n",
            "SUDDEN_SPEED_CHANGE:\n",
            "  - Detection rate: 100.00%\n",
            "  - Average score: 203.656097\n",
            "  - Score range: 3.675514 to 875.869873\n",
            "\n",
            "UNUSUAL_LOITERING:\n",
            "  - Detection rate: 100.00%\n",
            "  - Average score: 1.280210\n",
            "  - Score range: 0.240897 to 3.646321\n",
            "\n",
            "Saving enhanced model...\n",
            "\n",
            "Model saved successfully\n",
            "\n",
            "Enhanced model training and evaluation complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import json\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "class EnhancedFlightPredictionMLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes=[256, 128, 64, 32, 64, 128, 256]):\n",
        "        \"\"\"\n",
        "        Enhanced autoencoder with larger capacity and more sophisticated architecture\n",
        "\n",
        "        Args:\n",
        "            input_size: Number of input features\n",
        "            hidden_sizes: List of hidden layer sizes for the encoder and decoder\n",
        "        \"\"\"\n",
        "        super(EnhancedFlightPredictionMLP, self).__init__()\n",
        "\n",
        "        # Ensure symmetric architecture for proper autoencoder\n",
        "        self.input_size = input_size\n",
        "\n",
        "        # Build encoder layers\n",
        "        encoder_layers = []\n",
        "        prev_size = input_size\n",
        "        for h_size in hidden_sizes[:len(hidden_sizes)//2 + 1]:\n",
        "            encoder_layers.append(nn.Linear(prev_size, h_size))\n",
        "            encoder_layers.append(nn.BatchNorm1d(h_size))\n",
        "            encoder_layers.append(nn.LeakyReLU(0.2))\n",
        "            encoder_layers.append(nn.Dropout(0.25))\n",
        "            prev_size = h_size\n",
        "\n",
        "        self.encoder = nn.Sequential(*encoder_layers)\n",
        "\n",
        "        # Build decoder layers (reverse of encoder)\n",
        "        decoder_layers = []\n",
        "        for h_size in hidden_sizes[len(hidden_sizes)//2 + 1:]:\n",
        "            decoder_layers.append(nn.Linear(prev_size, h_size))\n",
        "            decoder_layers.append(nn.BatchNorm1d(h_size))\n",
        "            decoder_layers.append(nn.LeakyReLU(0.2))\n",
        "            decoder_layers.append(nn.Dropout(0.25))\n",
        "            prev_size = h_size\n",
        "\n",
        "        # Final layer to reconstruct input\n",
        "        decoder_layers.append(nn.Linear(prev_size, input_size))\n",
        "\n",
        "        self.decoder = nn.Sequential(*decoder_layers)\n",
        "\n",
        "        # Initialize weights for better gradient flow\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"Initialize weights using He initialization for better training\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through the autoencoder\"\"\"\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\"Get encoded representation\"\"\"\n",
        "        return self.encoder(x)\n",
        "\n",
        "def minimal_preprocess(filepath: str):\n",
        "    \"\"\"Minimal preprocessing - only handle format issues\"\"\"\n",
        "    with open(filepath, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    initial_count = len(df)\n",
        "    print(f\"\\nInitial entries: {initial_count}\")\n",
        "\n",
        "    features = ['alt', 'gs', 'heading', 'lat', 'lon', 'vertRate']\n",
        "\n",
        "    # Convert to numeric, keeping all values\n",
        "    for col in features:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "            nan_count = df[col].isna().sum()\n",
        "            if nan_count > 0:\n",
        "                print(f\"Found {nan_count} missing/invalid values in {col}\")\n",
        "                median_val = df[col].median()\n",
        "                df[col] = df[col].fillna(median_val)\n",
        "                print(f\"Filled missing values with median: {median_val}\")\n",
        "\n",
        "    # Handle altChange encoding\n",
        "    df['altChange'] = df['altChange'].fillna(0)\n",
        "    df['altChange_encoded'] = df['altChange'].map({' ': 0, 'C': 1, 'D': -1}).fillna(0)\n",
        "\n",
        "    # Ensure time column exists\n",
        "    if 'pitr' in df.columns and 'gs' in df.columns and len(df) > 1:\n",
        "        df = df.sort_values(['id', 'pitr']).reset_index(drop=True)\n",
        "\n",
        "        # Compute time difference in seconds\n",
        "        df['time_diff'] = df.groupby('id')['pitr'].diff().fillna(1)  # Avoid division by zero\n",
        "\n",
        "        # Compute rate of change (change per second)\n",
        "        df['gs_change_rate'] = df.groupby('id')['gs'].diff() / df['time_diff']\n",
        "        df['heading_change_rate'] = df.groupby('id')['heading'].diff().apply(lambda x: (x + 180) % 360 - 180) / df['time_diff']\n",
        "\n",
        "        # Replace infinity or extreme values with 0 or NaN\n",
        "        df['gs_change_rate'] = df['gs_change_rate'].replace([np.inf, -np.inf], 0)  # Replace infinity with 0\n",
        "        df['heading_change_rate'] = df['heading_change_rate'].replace([np.inf, -np.inf], 0)  # Replace infinity with 0\n",
        "\n",
        "        # Handle NaN values (if any remain after the replacement) by filling with 0\n",
        "        df['gs_change_rate'] = df['gs_change_rate'].fillna(0)\n",
        "        df['heading_change_rate'] = df['heading_change_rate'].fillna(0)\n",
        "\n",
        "    print(f\"\\nFeature engineering complete\")\n",
        "    print(df)\n",
        "\n",
        "    print(f\"\\nFinal entries: {len(df)}\")\n",
        "    return df\n",
        "\n",
        "def create_synthetic_anomalies(df, num_anomalies=1000):\n",
        "    \"\"\"Generate synthetic anomalies focusing on commercial flight patterns\"\"\"\n",
        "    anomalies = []\n",
        "\n",
        "    # Define commercial flight characteristics\n",
        "    COMMERCIAL_MIN_CRUISE_SPEED = 450  # in knots\n",
        "    COMMERCIAL_MAX_CRUISE_SPEED = 550  # in knots\n",
        "    COMMERCIAL_MIN_CRUISE_ALT = 30000  # in feet\n",
        "    COMMERCIAL_MAX_CRUISE_ALT = 40000  # in feet\n",
        "    COMMERCIAL_NORMAL_VERT_RATE = 2000  # in feet/min\n",
        "    COMMERCIAL_NORMAL_TURN_RATE = 3  # in degrees/second\n",
        "\n",
        "    # Define anomaly types with probabilities\n",
        "    anomaly_types = [\n",
        "        # Speed anomalies (40% chance)\n",
        "        {'type': 'high_speed', 'prob': 0.15},\n",
        "        {'type': 'low_speed_high_alt', 'prob': 0.15},\n",
        "        {'type': 'sudden_speed_change', 'prob': 0.10},\n",
        "\n",
        "        # Altitude anomalies (30% chance)\n",
        "        {'type': 'excessive_altitude', 'prob': 0.10},\n",
        "        {'type': 'low_altitude_high_speed', 'prob': 0.10},\n",
        "        {'type': 'rapid_altitude_drop', 'prob': 0.10},\n",
        "\n",
        "        # Heading anomalies (15% chance)\n",
        "        {'type': 'impossible_turn', 'prob': 0.05},\n",
        "        {'type': 'erratic_heading', 'prob': 0.10},\n",
        "\n",
        "        # Course & pattern anomalies (15% chance)\n",
        "        {'type': 'course_deviation', 'prob': 0.05},\n",
        "        {'type': 'unusual_loitering', 'prob': 0.05},\n",
        "        {'type': 'altitude_speed_fluctuations', 'prob': 0.05}\n",
        "    ]\n",
        "\n",
        "    # Calculate cumulative probabilities\n",
        "    cum_probs = np.cumsum([a['prob'] for a in anomaly_types])\n",
        "\n",
        "    for _ in range(num_anomalies):\n",
        "        # Sample a base flight\n",
        "        base_flight = df.sample(n=1).iloc[0].to_dict()\n",
        "        anomaly = base_flight.copy()\n",
        "\n",
        "        # Randomly select an anomaly type based on probabilities\n",
        "        rand_val = np.random.random()\n",
        "        anomaly_idx = np.searchsorted(cum_probs, rand_val)\n",
        "        anomaly_type = anomaly_types[anomaly_idx]['type']\n",
        "\n",
        "        # Apply the selected anomaly\n",
        "        if anomaly_type == 'high_speed':\n",
        "            anomaly['anomaly_type'] = anomaly_type\n",
        "            # Commercial aircraft flying faster than 600 knots\n",
        "            anomaly['gs'] = np.random.uniform(600, 1000)\n",
        "            #anomaly['gs_change'] = np.random.uniform(50, 100)  # Accelerating\n",
        "            anomalies.append(anomaly)\n",
        "\n",
        "        elif anomaly_type == 'low_speed_high_alt':\n",
        "            anomaly['anomaly_type'] = anomaly_type\n",
        "            # Aircraft flying below 200 knots at high altitude (stall risk)\n",
        "            anomaly['gs'] = np.random.uniform(100, 200)\n",
        "            anomaly['alt'] = np.random.uniform(35000, 40000)\n",
        "            #anomaly['gs_change'] = np.random.uniform(-50, -20)  # Decelerating\n",
        "            anomalies.append(anomaly)\n",
        "\n",
        "        elif anomaly_type == 'sudden_speed_change':\n",
        "            anomaly['anomaly_type'] = anomaly_type\n",
        "            # Sudden ±200 knots change (not physically possible)\n",
        "            direction = np.random.choice([-1, 1])\n",
        "            speed_change = np.random.uniform(200, 250) * direction\n",
        "\n",
        "            anomaly['gs'] = max(0, base_flight['gs'] + speed_change)  # Ensure non-negative speed\n",
        "            time_diff = np.random.uniform(1, 3)\n",
        "            # Ensure 'time_diff' exists and is valid\n",
        "            anomaly['gs_change_rate'] = speed_change / time_diff\n",
        "            anomalies.append(anomaly)\n",
        "\n",
        "\n",
        "        elif anomaly_type == 'excessive_altitude':\n",
        "            anomaly['anomaly_type'] = anomaly_type\n",
        "            # Commercial aircraft flying above 45,000 ft\n",
        "            anomaly['alt'] = np.random.uniform(45000, 60000)\n",
        "            #anomaly['vertRate'] = np.random.uniform(1000, 2000)  # Still climbing\n",
        "            anomalies.append(anomaly)\n",
        "\n",
        "        elif anomaly_type == 'low_altitude_high_speed':\n",
        "            anomaly['anomaly_type'] = anomaly_type\n",
        "            # High speed at low altitude\n",
        "            anomaly['alt'] = np.random.uniform(5000, 10000)\n",
        "            anomaly['gs'] = np.random.uniform(450, 550)\n",
        "            #anomaly['vertRate'] = np.random.uniform(-1000, 1000)  # Level or slight descent/climb\n",
        "            anomalies.append(anomaly)\n",
        "\n",
        "        elif anomaly_type == 'rapid_altitude_drop':\n",
        "            anomaly['anomaly_type'] = anomaly_type\n",
        "            # Rapid altitude drop (possible emergency)\n",
        "            anomaly['vertRate'] = np.random.uniform(-8000, -5000)\n",
        "            #anomaly['gs_change'] = np.random.uniform(-50, 50)  # Possible speed changes during emergency\n",
        "            anomalies.append(anomaly)\n",
        "\n",
        "        elif anomaly_type == 'impossible_turn':\n",
        "            anomaly['anomaly_type'] = anomaly_type\n",
        "            # Physically impossible turns for aircraft\n",
        "            direction = np.random.choice([-1, 1])  # Left (-1) or right (+1)\n",
        "            heading_change = direction * np.random.uniform(90, 120)  # Extreme turn rate\n",
        "\n",
        "            anomaly['heading'] = (base_flight['heading'] + heading_change) % 360\n",
        "            # Use a random time difference (1 to 5 seconds) to ensure a reasonable heading change rate\n",
        "            time_diff = np.random.uniform(1, 5)  # Time in seconds\n",
        "            anomaly['heading_change_rate'] = heading_change / time_diff  # Degrees per second\n",
        "            anomalies.append(anomaly)\n",
        "\n",
        "        elif anomaly_type == 'erratic_heading':\n",
        "            anomaly['anomaly_type'] = anomaly_type\n",
        "            # Create zigzag pattern in heading\n",
        "            direction = np.random.choice([-1, 1])  # Left (-1) or right (+1)\n",
        "            heading_change = direction * np.random.uniform(40, 80)  # Random change in range\n",
        "\n",
        "            anomaly['heading'] = (base_flight['heading'] + heading_change) % 360\n",
        "            time_diff = np.random.uniform(1, 2)\n",
        "            anomaly['heading_change_rate'] = heading_change / time_diff\n",
        "            anomalies.append(anomaly)\n",
        "\n",
        "        elif anomaly_type == 'course_deviation':\n",
        "            anomaly['anomaly_type'] = anomaly_type\n",
        "            # Significant deviation from expected route\n",
        "            deviation_magnitude = np.random.uniform(0.3, 0.7)  # roughly 20-40 NM\n",
        "            anomaly['lat'] = float(base_flight['lat']) + deviation_magnitude * np.random.choice([-1, 1])\n",
        "            anomaly['lon'] = float(base_flight['lon']) + deviation_magnitude * np.random.choice([-1, 1])\n",
        "\n",
        "            time_diff = np.random.uniform(1, 2)  # Time in seconds\n",
        "            heading_change = np.random.uniform(20, 40)  # Course correction\n",
        "            anomaly['heading_change_rate'] = heading_change / time_diff  # Degrees per second\n",
        "            anomalies.append(anomaly)\n",
        "\n",
        "        elif anomaly_type == 'unusual_loitering':\n",
        "            anomaly['anomaly_type'] = anomaly_type\n",
        "            # Aircraft circling or loitering unexpectedly\n",
        "            anomaly['gs'] = np.random.uniform(50, 100)\n",
        "            anomaly['vertRate'] = np.random.uniform(-100, 100)\n",
        "\n",
        "            time_diff = np.random.uniform(1, 2)  # Time in seconds\n",
        "            heading_change = np.random.uniform(5, 15)  # Slight turning\n",
        "            anomaly['heading_change_rate'] = heading_change / time_diff  # Degrees per second\n",
        "            anomalies.append(anomaly)\n",
        "\n",
        "        elif anomaly_type == 'altitude_speed_fluctuations':\n",
        "            time_diff = np.random.uniform(1, 5)  # Time in seconds\n",
        "            if np.random.random() > 0.5:\n",
        "                anomaly['anomaly_type'] = anomaly_type\n",
        "                # Climbing rapidly while speeding up\n",
        "                anomaly['vertRate'] = np.random.uniform(3000, 4000)\n",
        "                speed_change = np.random.uniform(50, 100)\n",
        "                anomaly['gs'] = float(base_flight['gs']) + speed_change\n",
        "                anomaly['gs_change_rate'] = speed_change / time_diff  # Knots per second\n",
        "            else:\n",
        "                # Descending rapidly while slowing down\n",
        "                anomaly['vertRate'] = np.random.uniform(-4000, -3000)\n",
        "                speed_change = np.random.uniform(50, 100)\n",
        "                anomaly['gs'] = max(0, float(base_flight['gs']) - speed_change)  # Ensure non-negative speed\n",
        "                anomaly['gs_change_rate'] = -speed_change / time_diff  # Knots per second (negative for slowing down)\n",
        "\n",
        "                # Add label for what type of anomaly this is (for analysis)\n",
        "                anomaly['anomaly_type'] = anomaly_type\n",
        "            anomalies.append(anomaly)\n",
        "\n",
        "\n",
        "    anomaly_df = pd.DataFrame(anomalies)\n",
        "\n",
        "    # Print statistics about the generated anomalies\n",
        "    anomaly_counts = anomaly_df['anomaly_type'].value_counts()\n",
        "    print(\"\\nSynthetic Commercial Flight Anomalies Created:\")\n",
        "    for anomaly_type, count in anomaly_counts.items():\n",
        "        print(f\"  - {anomaly_type}: {count} instances ({count/num_anomalies*100:.1f}%)\")\n",
        "\n",
        "    return anomaly_df\n",
        "\n",
        "class AnomalyDetector:\n",
        "    def __init__(self, model, scaler, threshold_multiplier=2.5):\n",
        "        self.model = model\n",
        "        self.scaler = scaler\n",
        "        self.threshold = None\n",
        "        self.threshold_multiplier = threshold_multiplier\n",
        "\n",
        "    def fit_threshold(self, normal_data):\n",
        "        \"\"\"Determine threshold using normal data with enhanced approach\"\"\"\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            normal_tensor = torch.FloatTensor(normal_data)\n",
        "            predictions = self.model(normal_tensor)\n",
        "            reconstruction_errors = torch.mean((normal_tensor - predictions) ** 2, dim=1)\n",
        "\n",
        "            # Use more sophisticated threshold calculation\n",
        "            # Based on percentile rather than just mean + std\n",
        "            sorted_errors = torch.sort(reconstruction_errors)[0]\n",
        "            percentile_95 = sorted_errors[int(0.95 * len(sorted_errors))]\n",
        "            percentile_99 = sorted_errors[int(0.99 * len(sorted_errors))]\n",
        "\n",
        "            # Use a weighted combination of statistics\n",
        "            self.threshold = 0.5 * percentile_95 + 0.5 * percentile_99\n",
        "\n",
        "            print(f\"\\nSet anomaly threshold at: {self.threshold:.6f}\")\n",
        "            print(f\"95th percentile: {percentile_95:.6f}\")\n",
        "            print(f\"99th percentile: {percentile_99:.6f}\")\n",
        "\n",
        "    def predict(self, data, return_scores=False):\n",
        "        \"\"\"Predict anomalies in new data\"\"\"\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            data_tensor = torch.FloatTensor(data)\n",
        "            predictions = self.model(data_tensor)\n",
        "            reconstruction_errors = torch.mean((data_tensor - predictions) ** 2, dim=1)\n",
        "\n",
        "            anomalies = reconstruction_errors > self.threshold\n",
        "\n",
        "            if return_scores:\n",
        "                return anomalies.numpy(), reconstruction_errors.numpy()\n",
        "            return anomalies.numpy()\n",
        "\n",
        "    def evaluate(self, normal_data, anomaly_data):\n",
        "        \"\"\"Evaluate detector performance\"\"\"\n",
        "        normal_predictions, normal_scores = self.predict(normal_data, return_scores=True)\n",
        "        false_positives = np.sum(normal_predictions)\n",
        "        false_positive_rate = (false_positives / len(normal_data)) * 100\n",
        "\n",
        "        anomaly_predictions, anomaly_scores = self.predict(anomaly_data, return_scores=True)\n",
        "        detected_anomalies = np.sum(anomaly_predictions)\n",
        "        detection_rate = (detected_anomalies / len(anomaly_data)) * 100\n",
        "\n",
        "        print(\"\\nAnomaly Detection Performance:\")\n",
        "        print(f\"False Positives: {false_positives} out of {len(normal_data)} normal points ({false_positive_rate:.2f}%)\")\n",
        "        print(f\"True Positives: {detected_anomalies} out of {len(anomaly_data)} anomalies ({detection_rate:.2f}%)\")\n",
        "        print(\"\\nReconstruction Error Statistics:\")\n",
        "        print(f\"Normal data - Mean: {np.mean(normal_scores):.6f}, Std: {np.std(normal_scores):.6f}\")\n",
        "        print(f\"Anomaly data - Mean: {np.mean(anomaly_scores):.6f}, Std: {np.std(anomaly_scores):.6f}\")\n",
        "\n",
        "        # Analyze performance by anomaly type\n",
        "        if 'anomaly_type' in anomaly_data:\n",
        "            print(\"\\nDetection Rate by Anomaly Type:\")\n",
        "            anomaly_df = pd.DataFrame({\n",
        "                'anomaly_type': anomaly_data['anomaly_type'],\n",
        "                'is_detected': anomaly_predictions,\n",
        "                'score': anomaly_scores\n",
        "            })\n",
        "\n",
        "            for anomaly_type, group in anomaly_df.groupby('anomaly_type'):\n",
        "                detection_rate = (group['is_detected'].sum() / len(group)) * 100\n",
        "                avg_score = group['score'].mean()\n",
        "                print(f\"  - {anomaly_type}: {detection_rate:.2f}% detected (avg score: {avg_score:.6f})\")\n",
        "\n",
        "        return {\n",
        "            'false_positive_rate': false_positive_rate,\n",
        "            'detection_rate': detection_rate,\n",
        "            'normal_scores': normal_scores,\n",
        "            'anomaly_scores': anomaly_scores\n",
        "        }\n",
        "\n",
        "def train_model(model, train_data, val_data, epochs=300, batch_size=128):\n",
        "    \"\"\"Train model with improved training loop and learning rate scheduling\"\"\"\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "    # Multi-step learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(\n",
        "        optimizer,\n",
        "        milestones=[30, 60, 90, 120],\n",
        "        gamma=0.5\n",
        "    )\n",
        "\n",
        "    train_tensor = torch.FloatTensor(train_data)\n",
        "    val_tensor = torch.FloatTensor(val_data)\n",
        "\n",
        "    # For early stopping\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "    patience = 100  # Increased patience for larger model\n",
        "    patience_counter = 0\n",
        "\n",
        "    # Training history\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        batch_count = 0\n",
        "\n",
        "        # Create random permutation for batch sampling\n",
        "        indices = torch.randperm(len(train_tensor))\n",
        "\n",
        "        for i in range(0, len(train_tensor), batch_size):\n",
        "            # Get batch using permutation\n",
        "            batch_indices = indices[i:i+batch_size]\n",
        "            batch = train_tensor[batch_indices]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch)\n",
        "            loss = criterion(outputs, batch)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            batch_count += 1\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(val_tensor)\n",
        "            val_loss = criterion(val_outputs, val_tensor)\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step()\n",
        "\n",
        "        # Save losses for plotting\n",
        "        avg_train_loss = total_loss / batch_count\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(val_loss.item())\n",
        "\n",
        "        # Early stopping check\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_state = model.state_dict().copy()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.6f}, Val Loss: {val_loss:.6f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n",
        "\n",
        "    # Restore best model\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    return model, train_losses, val_losses\n",
        "\n",
        "def save_model(model, scaler, detector, save_dir='saved_models'):\n",
        "    \"\"\"Save the trained model, scaler, and detector configuration\"\"\"\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    model_path = os.path.join(save_dir, 'flight_anomaly_model.pth')\n",
        "    scaler_path = os.path.join(save_dir, 'scaler.joblib')\n",
        "    detector_path = os.path.join(save_dir, 'detector.joblib')\n",
        "\n",
        "    # Save the PyTorch model\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'input_size': model.input_size,\n",
        "        'model_type': 'EnhancedFlightPredictionMLP'\n",
        "    }, model_path)\n",
        "\n",
        "    # Save the scaler\n",
        "    joblib.dump(scaler, scaler_path)\n",
        "\n",
        "    # Save detector configuration\n",
        "    detector_config = {\n",
        "        'threshold': detector.threshold.item() if detector.threshold is not None else None,\n",
        "        'threshold_multiplier': detector.threshold_multiplier\n",
        "    }\n",
        "    joblib.dump(detector_config, detector_path)\n",
        "\n",
        "    print(f\"\\nModel saved successfully\")\n",
        "\n",
        "def load_model(model_path='saved_models/flight_anomaly_model.pth',\n",
        "              scaler_path='saved_models/scaler.joblib',\n",
        "              detector_path='saved_models/detector.joblib'):\n",
        "    \"\"\"Load the saved model, scaler, and detector configuration\"\"\"\n",
        "    # Load model\n",
        "    checkpoint = torch.load(model_path)\n",
        "    model_type = checkpoint.get('model_type', 'FlightPredictionMLP')\n",
        "\n",
        "    if model_type == 'EnhancedFlightPredictionMLP':\n",
        "        model = EnhancedFlightPredictionMLP(input_size=checkpoint['input_size'])\n",
        "    else:\n",
        "        # Fallback for compatibility with older models\n",
        "        from collections import OrderedDict\n",
        "        model = FlightPredictionMLP(input_size=checkpoint['input_size'])\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    # Load scaler\n",
        "    scaler = joblib.load(scaler_path)\n",
        "\n",
        "    # Load detector configuration\n",
        "    detector_config = joblib.load(detector_path)\n",
        "    detector = AnomalyDetector(model, scaler, detector_config['threshold_multiplier'])\n",
        "    detector.threshold = torch.tensor(detector_config['threshold'])\n",
        "\n",
        "    print(f\"\\nModel loaded successfully: {model_type}\")\n",
        "    return model, scaler, detector\n",
        "\n",
        "def analyze_anomaly_scores(anomaly_df, scores, detector_threshold):\n",
        "    \"\"\"Analyze detection performance for different types of anomalies\"\"\"\n",
        "    anomaly_df['score'] = scores\n",
        "    anomaly_df['is_detected'] = scores > detector_threshold\n",
        "\n",
        "    print(\"\\nAnomaly Detection Analysis by Type:\")\n",
        "    for anomaly_type, group in anomaly_df.groupby('anomaly_type'):\n",
        "        detection_rate = (group['is_detected'].sum() / len(group)) * 100\n",
        "        avg_score = group['score'].mean()\n",
        "        max_score = group['score'].max()\n",
        "        min_score = group['score'].min()\n",
        "\n",
        "        print(f\"\\n{anomaly_type.upper()}:\")\n",
        "        print(f\"  - Detection rate: {detection_rate:.2f}%\")\n",
        "        print(f\"  - Average score: {avg_score:.6f}\")\n",
        "        print(f\"  - Score range: {min_score:.6f} to {max_score:.6f}\")\n",
        "\n",
        "        # Find hardest to detect examples\n",
        "        if not group['is_detected'].all() and len(group) > 1:\n",
        "            missed = group[~group['is_detected']].sort_values('score', ascending=True)\n",
        "            if len(missed) > 0:\n",
        "                print(f\"  - Hardest to detect example (score: {missed.iloc[0]['score']:.6f}):\")\n",
        "                for feature in ['alt', 'gs', 'heading', 'vertRate','gs_change_rate','heading_change_rate']:\n",
        "                    if feature in missed.columns:\n",
        "                        print(f\"    {feature}: {missed.iloc[0][feature]}\")\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # 1. Load and preprocess normal data\n",
        "        print(\"Loading and preprocessing data...\")\n",
        "        df = minimal_preprocess('/content/drive/MyDrive/data_MVP.json')\n",
        "        # 2. Prepare features\n",
        "        features = ['alt', 'gs', 'heading', 'lat', 'lon', 'vertRate', 'altChange_encoded']\n",
        "        # Add new engineered features if they exist\n",
        "        if 'gs_change_rate' in df.columns:\n",
        "            features.append('gs_change_rate')\n",
        "        if 'heading_change_rate' in df.columns:\n",
        "            features.append('heading_change_rate')\n",
        "\n",
        "        X = df[features].values\n",
        "\n",
        "        # 3. Scale the features\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "        # 4. Split into train/validation sets\n",
        "        X_train, X_val = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "        # 5. Create and train the enhanced model\n",
        "        print(\"\\nTraining enhanced model with larger architecture...\")\n",
        "        model = EnhancedFlightPredictionMLP(input_size=len(features))\n",
        "        model, train_losses, val_losses = train_model(model, X_train, X_val, epochs=300, batch_size=128)\n",
        "        #model, scaler, detector = load_model()\n",
        "        # 6. Generate synthetic commercial flight anomalies\n",
        "        print(\"\\nGenerating synthetic commercial flight anomalies...\")\n",
        "        anomaly_df = create_synthetic_anomalies(df, num_anomalies=1000)  # Increased number of anomalies\n",
        "\n",
        "        # Create a dataframe for analysis that includes the anomaly type\n",
        "        anomaly_types = anomaly_df['anomaly_type'].copy()\n",
        "        print(anomaly_types)\n",
        "        # Extract only the features for anomaly detection\n",
        "        anomaly_features = anomaly_df[features].values\n",
        "        anomaly_features_scaled = scaler.transform(anomaly_features)\n",
        "\n",
        "        # 7. Create and evaluate anomaly detector with adjusted threshold\n",
        "        print(\"\\nEvaluating commercial flight anomaly detection...\")\n",
        "        detector = AnomalyDetector(model, scaler, threshold_multiplier=2.5)  # Adjusted threshold\n",
        "        detector.fit_threshold(X_train)\n",
        "\n",
        "        # Basic evaluation\n",
        "        results = detector.evaluate(X_val, anomaly_features_scaled)\n",
        "\n",
        "        # 8. Detailed analysis by anomaly type\n",
        "        print(\"\\nPerforming detailed analysis by anomaly type...\")\n",
        "        with torch.no_grad():\n",
        "            anomaly_tensor = torch.FloatTensor(anomaly_features_scaled)\n",
        "            predictions = model(anomaly_tensor)\n",
        "            reconstruction_errors = torch.mean((anomaly_tensor - predictions) ** 2, dim=1).numpy()\n",
        "\n",
        "        # Add the anomaly type back for analysis\n",
        "        anomaly_analysis_df = pd.DataFrame({\n",
        "            'anomaly_type': anomaly_types,\n",
        "            'score': reconstruction_errors,\n",
        "            'is_detected': reconstruction_errors > detector.threshold.item()\n",
        "        })\n",
        "\n",
        "        # Include original feature values for analysis\n",
        "        for col in features:\n",
        "            if col in anomaly_df.columns:\n",
        "                anomaly_analysis_df[col] = anomaly_df[col].values\n",
        "\n",
        "        analyze_anomaly_scores(anomaly_analysis_df, reconstruction_errors, detector.threshold.item())\n",
        "\n",
        "        # 9. Save model and components\n",
        "        print(\"\\nSaving enhanced model...\")\n",
        "        save_model(model, scaler, detector)\n",
        "\n",
        "        # Optionally load the model back to verify\n",
        "        #\n",
        "\n",
        "        print(\"\\nEnhanced model training and evaluation complete!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main execution: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pI61D1ZXiIqK",
        "outputId": "28444657-e851-41bc-fe99-06e614c2f915"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preprocessing data...\n",
            "\n",
            "Initial entries: 39092\n",
            "Found 5507 missing/invalid values in alt\n",
            "Filled missing values with median: 32275.0\n",
            "Found 5491 missing/invalid values in gs\n",
            "Filled missing values with median: 416.0\n",
            "Found 3 missing/invalid values in heading\n",
            "Filled missing values with median: 160.0\n",
            "Found 1 missing/invalid values in lat\n",
            "Filled missing values with median: 25.54697\n",
            "Found 1 missing/invalid values in lon\n",
            "Filled missing values with median: 50.64428\n",
            "Found 5850 missing/invalid values in vertRate\n",
            "Filled missing values with median: 0.0\n",
            "\n",
            "Feature engineering complete\n",
            "               pitr      alt altChange                        id     gs  \\\n",
            "0      1.682017e+09    600.0            4LMWC-1682015398-adhoc-0  168.0   \n",
            "1      1.682017e+09   2025.0         C  4LMWC-1682015398-adhoc-0  212.0   \n",
            "2      1.682017e+09   2175.0         C  4LMWC-1682015398-adhoc-0  246.0   \n",
            "3      1.682017e+09   2150.0            4LMWC-1682015398-adhoc-0  256.0   \n",
            "4      1.682017e+09   2150.0            4LMWC-1682015398-adhoc-0  259.0   \n",
            "...             ...      ...       ...                       ...    ...   \n",
            "39087  1.682019e+09  37000.0            YRBEE-1682010317-adhoc-0  465.0   \n",
            "39088  1.682019e+09  37000.0            YRBEE-1682010317-adhoc-0  462.0   \n",
            "39089  1.682019e+09  36975.0            YRBEE-1682010317-adhoc-0  461.0   \n",
            "39090  1.682019e+09  36950.0            YRBEE-1682010317-adhoc-0  462.0   \n",
            "39091           NaN  32275.0         0                       NaN  416.0   \n",
            "\n",
            "       heading       lat       lon  vertRate  altChange_encoded  time_diff  \\\n",
            "0        303.0  25.33580  55.50985       0.0                0.0        1.0   \n",
            "1        302.0  25.36281  55.46254     640.0                1.0       61.0   \n",
            "2        302.0  25.37930  55.43359       0.0                1.0       27.0   \n",
            "3        302.0  25.38940  55.41597       0.0                0.0       17.0   \n",
            "4        303.0  25.40793  55.38338       0.0                0.0       29.0   \n",
            "...        ...       ...       ...       ...                ...        ...   \n",
            "39087    144.0  29.59265  31.81344       0.0                0.0       48.0   \n",
            "39088    143.0  29.53871  31.85887    -448.0                0.0       31.0   \n",
            "39089    144.0  29.46881  31.91782     -64.0                0.0       38.0   \n",
            "39090    144.0  29.41692  31.96138       0.0                0.0       39.0   \n",
            "39091    160.0  25.54697  50.64428       0.0                0.0        1.0   \n",
            "\n",
            "       gs_change_rate  heading_change_rate  \n",
            "0            0.000000             0.000000  \n",
            "1            0.721311            -0.016393  \n",
            "2            1.259259             0.000000  \n",
            "3            0.588235             0.000000  \n",
            "4            0.103448             0.034483  \n",
            "...               ...                  ...  \n",
            "39087       -0.083333             0.000000  \n",
            "39088       -0.096774            -0.032258  \n",
            "39089       -0.026316             0.026316  \n",
            "39090        0.025641             0.000000  \n",
            "39091        0.000000             0.000000  \n",
            "\n",
            "[39092 rows x 13 columns]\n",
            "\n",
            "Final entries: 39092\n",
            "\n",
            "Training enhanced model with larger architecture...\n",
            "Epoch [10/300], Train Loss: 0.245839, Val Loss: 0.081305, LR: 0.001000\n",
            "Epoch [20/300], Train Loss: 0.186360, Val Loss: 0.129023, LR: 0.001000\n",
            "Epoch [30/300], Train Loss: 0.160967, Val Loss: 0.073647, LR: 0.000500\n",
            "Epoch [40/300], Train Loss: 0.149465, Val Loss: 0.051078, LR: 0.000500\n",
            "Epoch [50/300], Train Loss: 0.144578, Val Loss: 0.064031, LR: 0.000500\n",
            "Epoch [60/300], Train Loss: 0.141556, Val Loss: 0.084529, LR: 0.000250\n",
            "Epoch [70/300], Train Loss: 0.142862, Val Loss: 0.057878, LR: 0.000250\n",
            "Epoch [80/300], Train Loss: 0.134663, Val Loss: 0.050808, LR: 0.000250\n",
            "Epoch [90/300], Train Loss: 0.135104, Val Loss: 0.054818, LR: 0.000125\n",
            "Epoch [100/300], Train Loss: 0.130789, Val Loss: 0.058921, LR: 0.000125\n",
            "Epoch [110/300], Train Loss: 0.135622, Val Loss: 0.046464, LR: 0.000125\n",
            "Epoch [120/300], Train Loss: 0.129847, Val Loss: 0.053021, LR: 0.000063\n",
            "Epoch [130/300], Train Loss: 0.131067, Val Loss: 0.083966, LR: 0.000063\n",
            "Epoch [140/300], Train Loss: 0.131492, Val Loss: 0.043589, LR: 0.000063\n",
            "Epoch [150/300], Train Loss: 0.127929, Val Loss: 0.060944, LR: 0.000063\n",
            "Epoch [160/300], Train Loss: 0.129428, Val Loss: 0.057662, LR: 0.000063\n",
            "Epoch [170/300], Train Loss: 0.128764, Val Loss: 0.046182, LR: 0.000063\n",
            "Epoch [180/300], Train Loss: 0.132987, Val Loss: 0.046433, LR: 0.000063\n",
            "Epoch [190/300], Train Loss: 0.128120, Val Loss: 0.045643, LR: 0.000063\n",
            "Epoch [200/300], Train Loss: 0.129445, Val Loss: 0.053919, LR: 0.000063\n",
            "Epoch [210/300], Train Loss: 0.126819, Val Loss: 0.049094, LR: 0.000063\n",
            "Epoch [220/300], Train Loss: 0.131534, Val Loss: 0.057179, LR: 0.000063\n",
            "Epoch [230/300], Train Loss: 0.127512, Val Loss: 0.061616, LR: 0.000063\n",
            "Epoch [240/300], Train Loss: 0.127898, Val Loss: 0.044925, LR: 0.000063\n",
            "Epoch [250/300], Train Loss: 0.127440, Val Loss: 0.068468, LR: 0.000063\n",
            "Epoch [260/300], Train Loss: 0.129881, Val Loss: 0.053974, LR: 0.000063\n",
            "Epoch [270/300], Train Loss: 0.130040, Val Loss: 0.051615, LR: 0.000063\n",
            "Epoch [280/300], Train Loss: 0.123773, Val Loss: 0.041047, LR: 0.000063\n",
            "Epoch [290/300], Train Loss: 0.127068, Val Loss: 0.053365, LR: 0.000063\n",
            "Epoch [300/300], Train Loss: 0.122711, Val Loss: 0.040445, LR: 0.000063\n",
            "\n",
            "Generating synthetic commercial flight anomalies...\n",
            "\n",
            "Synthetic Commercial Flight Anomalies Created:\n",
            "  - high_speed: 171 instances (17.1%)\n",
            "  - low_speed_high_alt: 137 instances (13.7%)\n",
            "  - erratic_heading: 117 instances (11.7%)\n",
            "  - excessive_altitude: 99 instances (9.9%)\n",
            "  - sudden_speed_change: 91 instances (9.1%)\n",
            "  - low_altitude_high_speed: 88 instances (8.8%)\n",
            "  - rapid_altitude_drop: 82 instances (8.2%)\n",
            "  - unusual_loitering: 62 instances (6.2%)\n",
            "  - impossible_turn: 57 instances (5.7%)\n",
            "  - course_deviation: 48 instances (4.8%)\n",
            "  - altitude_speed_fluctuations: 48 instances (4.8%)\n",
            "0               excessive_altitude\n",
            "1              sudden_speed_change\n",
            "2                  impossible_turn\n",
            "3          low_altitude_high_speed\n",
            "4          low_altitude_high_speed\n",
            "                  ...             \n",
            "995        low_altitude_high_speed\n",
            "996                     high_speed\n",
            "997             low_speed_high_alt\n",
            "998                impossible_turn\n",
            "999    altitude_speed_fluctuations\n",
            "Name: anomaly_type, Length: 1000, dtype: object\n",
            "\n",
            "Evaluating commercial flight anomaly detection...\n",
            "\n",
            "Set anomaly threshold at: 0.160073\n",
            "95th percentile: 0.106618\n",
            "99th percentile: 0.213529\n",
            "\n",
            "Anomaly Detection Performance:\n",
            "False Positives: 152 out of 7819 normal points (1.94%)\n",
            "True Positives: 955 out of 1000 anomalies (95.50%)\n",
            "\n",
            "Reconstruction Error Statistics:\n",
            "Normal data - Mean: 0.040445, Std: 0.421635\n",
            "Anomaly data - Mean: 19.248249, Std: 55.683445\n",
            "\n",
            "Performing detailed analysis by anomaly type...\n",
            "\n",
            "Anomaly Detection Analysis by Type:\n",
            "\n",
            "ALTITUDE_SPEED_FLUCTUATIONS:\n",
            "  - Detection rate: 100.00%\n",
            "  - Average score: 12.195324\n",
            "  - Score range: 1.451961 to 78.168358\n",
            "\n",
            "COURSE_DEVIATION:\n",
            "  - Detection rate: 100.00%\n",
            "  - Average score: 14.436603\n",
            "  - Score range: 4.056957 to 36.433132\n",
            "\n",
            "ERRATIC_HEADING:\n",
            "  - Detection rate: 100.00%\n",
            "  - Average score: 44.235256\n",
            "  - Score range: 8.053890 to 151.440231\n",
            "\n",
            "EXCESSIVE_ALTITUDE:\n",
            "  - Detection rate: 72.73%\n",
            "  - Average score: 0.312669\n",
            "  - Score range: 0.028152 to 1.282129\n",
            "  - Hardest to detect example (score: 0.028152):\n",
            "    alt: 45160.912177665894\n",
            "    gs: 508.0\n",
            "    heading: 138.0\n",
            "    vertRate: 0.0\n",
            "    gs_change_rate: 0.0\n",
            "    heading_change_rate: 0.0\n",
            "\n",
            "HIGH_SPEED:\n",
            "  - Detection rate: 90.06%\n",
            "  - Average score: 1.014651\n",
            "  - Score range: 0.057707 to 3.523730\n",
            "  - Hardest to detect example (score: 0.057707):\n",
            "    alt: 34600.0\n",
            "    gs: 626.4816001437848\n",
            "    heading: 117.0\n",
            "    vertRate: 704.0\n",
            "    gs_change_rate: -0.13333333333333333\n",
            "    heading_change_rate: 0.0\n",
            "\n",
            "IMPOSSIBLE_TURN:\n",
            "  - Detection rate: 100.00%\n",
            "  - Average score: 49.800770\n",
            "  - Score range: 4.919020 to 247.069687\n",
            "\n",
            "LOW_ALTITUDE_HIGH_SPEED:\n",
            "  - Detection rate: 98.86%\n",
            "  - Average score: 0.379909\n",
            "  - Score range: 0.144188 to 0.604746\n",
            "  - Hardest to detect example (score: 0.144188):\n",
            "    alt: 9505.799922098053\n",
            "    gs: 472.18162224466596\n",
            "    heading: 110.0\n",
            "    vertRate: 1280.0\n",
            "    gs_change_rate: 0.06060606060606061\n",
            "    heading_change_rate: 0.0\n",
            "\n",
            "LOW_SPEED_HIGH_ALT:\n",
            "  - Detection rate: 100.00%\n",
            "  - Average score: 0.597006\n",
            "  - Score range: 0.262051 to 1.382759\n",
            "\n",
            "RAPID_ALTITUDE_DROP:\n",
            "  - Detection rate: 100.00%\n",
            "  - Average score: 1.186724\n",
            "  - Score range: 0.203144 to 3.063115\n",
            "\n",
            "SUDDEN_SPEED_CHANGE:\n",
            "  - Detection rate: 100.00%\n",
            "  - Average score: 103.554451\n",
            "  - Score range: 3.825208 to 684.088257\n",
            "\n",
            "UNUSUAL_LOITERING:\n",
            "  - Detection rate: 100.00%\n",
            "  - Average score: 1.859666\n",
            "  - Score range: 0.335965 to 16.224754\n",
            "\n",
            "Saving enhanced model...\n",
            "\n",
            "Model saved successfully\n",
            "\n",
            "Enhanced model training and evaluation complete!\n"
          ]
        }
      ]
    }
  ]
}